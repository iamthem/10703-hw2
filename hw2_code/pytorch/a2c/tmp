
            #logger.debug('G times NLL ===> %s', str(loss_train))
            # states, actions, rewards, policy_outputs, T = Reinforce_net.generate_episode(env, batch, sampling = True)
            # G = Reinforce_net.naiveGt(gamma, T, torch.zeros((T)), rewards)
            # nll_loss = Reinforce_net.loss(policy_outputs, actions, G, T)
            # logger.debug("States (%s) => %s\npolicy_outputs (%s) ==> %s\n actions (%s) ==> %s\nG (%s) ==> %s\n \nLoss ==> %s", str(states.shape), str(states), str(policy_outputs.shape), str(torch.exp(policy_outputs)), str(actions.shape), str(actions), str(G.shape), str(G), str(nll_loss))
            
            # if m % 100 == 0:
            #     G = np.zeros(test_episodes)
            #     Loss = np.zeros(test_episodes)  

            #     for k in range(test_episodes):
            #         g, l, actions, states, policy_outputs = Reinforce_net.evaluate_policy(env, batch)
            #         # diffs = [round(float(G[i] - G[i + 1]), 4) for i in range(1, len(G) - 1)]
            #         # logger.debug("G.shape ==> %s\n lenght of G_diffs = %d, \tG_diffs ==> %s", 
            #         #              str(G.shape), len(diffs), str(diffs))
            #         G[k] = g
            #         Loss[k] = l

            #     reward_mean = G.mean()
            #     reward_sd = G.std()
            #     Loss_mean = Loss.mean()
            #     Loss_sd = G.std()
                #hists(Loss, Loss_mean, G, reward_mean, m, reward_sd, Loss_sd)
                #totals = zip(states, actions.tolist())
                #logger.debug("outputs (%s) %s\n totals => %s, \nloss_mean %f", str(policy_outputs.shape), str(policy_outputs), str(list(totals)), Loss_mean)
