
            #logger.debug('G times NLL ===> %s', str(loss_train))
            # states, actions, rewards, policy_outputs, T = Reinforce_net.generate_episode(env, batch, sampling = True)
            # G = Reinforce_net.naiveGt(gamma, T, torch.zeros((T)), rewards)
            # nll_loss = Reinforce_net.loss(policy_outputs, actions, G, T)
            # logger.debug("States (%s) => %s\npolicy_outputs (%s) ==> %s\n actions (%s) ==> %s\nG (%s) ==> %s\n \nLoss ==> %s", str(states.shape), str(states), str(policy_outputs.shape), str(torch.exp(policy_outputs)), str(actions.shape), str(actions), str(G.shape), str(G), str(nll_loss))
            
